#!/usr/bin/env bash
# SIG: mon-recoweb vMini-perext-whatweb-2025-09-05-impr3

set -euo pipefail

# ---------- Couleurs ----------
C_G="\e[32m"   # vert
C_B="\e[34m"   # bleu
C_HI="\e[1m"   # bold
C_RST="\e[0m"  # reset

# ---------- Defaults (surchargables) ----------
DIR_WORDLIST="${DIR_WORDLIST:-/usr/share/seclists/Discovery/Web-Content/common.txt}"
EXT_WORDLIST="${EXT_WORDLIST:-/usr/share/seclists/Discovery/Web-Content/raft-medium-extensions-lowercase.txt}"
EXTS="${EXTS:-php,html,txt}"  # extensions traitées pour la phase "files"

# ---------- Args ----------
if [[ $# -lt 1 ]]; then
  echo "Usage: $(basename "$0") <cible ex: planning.htb|http(s)://planning.htb/>" >&2
  exit 1
fi
TARGET_RAW="$1"

# ---------- Normalisation URL ----------
norm_url() {
  local u="$1"
  u="${u%/}"
  if [[ ! "$u" =~ ^https?:// ]]; then
    u="http://$u"
  fi
  echo "${u}/"
}
BASE_URL="$(norm_url "$TARGET_RAW")"
HOST="$(awk -F[/:] '{print $4}' <<< "$BASE_URL")"

# ---------- Dossier de sortie ----------
OUTDIR="mon-recoweb_${HOST}"
mkdir -p "$OUTDIR"

echo -e "${C_G}[✓]${C_RST} Dossier : ${OUTDIR}"
echo -e "${C_G}[✓]${C_RST} Base URL : ${BASE_URL%/}"

# ---------- Vérifs outillage minimal ----------
command -v ffuf >/dev/null 2>&1 || { echo "ffuf est requis." >&2; exit 2; }
command -v whatweb >/dev/null 2>&1 || { echo "whatweb est requis." >&2; exit 2; }
command -v curl >/dev/null 2>&1 || { echo "curl est requis." >&2; exit 2; }

# ---------- Phase 1 : WhatWeb ----------
echo -e "${C_B}Lancement de WhatWeb...${C_RST}"
: > "${OUTDIR}/whatweb.txt"
whatweb --no-errors --log-brief="${OUTDIR}/whatweb.txt" "${BASE_URL%/}" >/dev/null 2>&1 || true

echo -e "${C_HI}=== WHATWEB ===${C_RST}"
if [[ -s "${OUTDIR}/whatweb.txt" ]]; then
  cat "${OUTDIR}/whatweb.txt"
else
  echo "Aucun résultat (whatweb)."
fi
echo

# ---------- Helper : CSV ffuf -> lignes lisibles ----------
format_ffuf_csv_to_lines() {
  # attend CSV ffuf (en-tête: status,length,words,lines,duration,url/URL,redirectlocation)
  # imprime: "CODE  len=XXXX    URL"
  awk -F',' '
    NR==1 {
      for (i=1; i<=NF; i++) {
        key=$i
        gsub(/^"|"$/, "", key)
        h[key]=i
      }
      next
    }
    {
      # indices robustes selon la casse utilisée par ffuf
      is = (("status" in h) ? h["status"] : (("Status" in h) ? h["Status"] : 0))
      il = (("length" in h) ? h["length"] : (("Length" in h) ? h["Length"] : 0))
      iu = (("url"    in h) ? h["url"]    : (("URL"    in h) ? h["URL"]    : (("Url" in h) ? h["Url"] : 0)))

      code = (is ? $is : "")
      length = (il ? $il : "")
      url = (iu ? $iu : "")

      gsub(/^"|"$/, "", code)
      gsub(/^"|"$/, "", length)
      gsub(/^"|"$/, "", url)

      if (url != "") {
        printf "%s  len=%s    %s\n", code, length, url
      }
    }' "$1"
}

# ---------- Phase 2 : ffuf répertoires ----------
echo -e "${C_B}Lancement de ffuf répertoires...${C_RST}"
CSV_DIRS="${OUTDIR}/dirs.csv"; : > "$CSV_DIRS"
ffuf -u "${BASE_URL}FUZZ" -w "$DIR_WORDLIST" -of csv -o "$CSV_DIRS" -fc 404 -sa -t 40 >/dev/null 2>&1 || true

# Unicité par URL (dernière colonne)
awk -F',' 'NR==1{print;next} !seen[$NF]++' "$CSV_DIRS" > "${CSV_DIRS}.uniq" && mv "${CSV_DIRS}.uniq" "$CSV_DIRS"

SUMMARY_DIRS="${OUTDIR}/summary_dirs.txt"
format_ffuf_csv_to_lines "$CSV_DIRS" > "$SUMMARY_DIRS" || true

# Filet de sécurité : s assurer que "/" figure côté répertoires si la racine répond 200
root_code=$(curl -sk -o /dev/null -w '%{http_code}' "${BASE_URL}") || root_code=0
if [[ "$root_code" == "200" ]]; then
  if ! grep -qE '/\s*$' "$SUMMARY_DIRS" 2>/dev/null; then
    echo -e "200  len=?    ${BASE_URL}" >> "$SUMMARY_DIRS"
    awk '!seen[$0]++' "$SUMMARY_DIRS" > "${SUMMARY_DIRS}.uniq" && mv "${SUMMARY_DIRS}.uniq" "$SUMMARY_DIRS"
  fi
fi

# Bilan chiffré (dirs)
echo -e "${C_HI}=== RÉPERTOIRES TROUVÉS (${C_RST}$(wc -l < "$SUMMARY_DIRS" 2>/dev/null || echo 0)${C_HI}) ===${C_RST}"
if [[ -s "$SUMMARY_DIRS" ]]; then
  cat "$SUMMARY_DIRS"
else
  echo "Aucun répertoire trouvé."
fi
echo

# ---------- Phase 3 : ffuf extensions (files) ----------
echo -e "${C_B}Lancement de ffuf extensions...${C_RST}"
CSV_FILES="${OUTDIR}/files.csv"; : > "$CSV_FILES"
# Extensions traitées (php,html,txt par défaut)
ffuf -u "${BASE_URL}FUZZ" -w "$EXT_WORDLIST" -of csv -o "$CSV_FILES" -fc 404 -sa -t 40 -e "$(tr -d ' ' <<< "$EXTS" | tr ',' ',')" >/dev/null 2>&1 || true

# Unicité par URL (dernière colonne)
awk -F',' 'NR==1{print;next} !seen[$NF]++' "$CSV_FILES" > "${CSV_FILES}.uniq" && mv "${CSV_FILES}.uniq" "$CSV_FILES"

SUMMARY_FILES="${OUTDIR}/summary_files.txt"
format_ffuf_csv_to_lines "$CSV_FILES" > "$SUMMARY_FILES" || true

# Dé-doublonnage : retirer /index.php si "/" existe côté dirs
if grep -qE '/\s*$' "$SUMMARY_DIRS" 2>/dev/null && grep -q '/index\.php$' "$SUMMARY_FILES" 2>/dev/null; then
  sed -i '/\/index\.php$/d' "$SUMMARY_FILES"
fi

# --- robots.txt & sitemap.xml (si 200) : enrichit summary_files.txt ---
for extra in robots.txt sitemap.xml; do
  code=$(curl -sk -o /dev/null -w '%{http_code}' "${BASE_URL}${extra}") || code=0
  if [[ "$code" == "200" ]]; then
    echo -e "200  len=?    ${BASE_URL}${extra}" >> "$SUMMARY_FILES"
  fi
done
# Unicité finale du fichier (au cas où)
awk '!seen[$0]++' "$SUMMARY_FILES" > "${SUMMARY_FILES}.uniq" && mv "${SUMMARY_FILES}.uniq" "$SUMMARY_FILES"

# Bilan chiffré (files)
echo -e "${C_HI}=== FICHIERS TROUVÉS (ext: ${EXTS} | ${C_RST}$(wc -l < "$SUMMARY_FILES" 2>/dev/null || echo 0)${C_HI}) ===${C_RST}"
if [[ -s "$SUMMARY_FILES" ]]; then
  cat "$SUMMARY_FILES"
else
  echo "Aucun fichier trouvé."
fi
echo

# ---------- Récapitulatif final ----------
echo -e "${C_G}[✓]${C_RST} ${C_B}Terminé.${C_RST}"
echo "  -> ${OUTDIR}/whatweb.txt ($(wc -l < "${OUTDIR}/whatweb.txt" 2>/dev/null || echo 0) ligne)"
echo "  -> ${OUTDIR}/summary_dirs.txt ($(wc -l < "${SUMMARY_DIRS}" 2>/dev/null || echo 0) lignes)"
echo "  -> ${OUTDIR}/summary_files.txt ($(wc -l < "${SUMMARY_FILES}" 2>/dev/null || echo 0) lignes)"

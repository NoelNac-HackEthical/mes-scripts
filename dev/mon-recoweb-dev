#!/usr/bin/env bash
# NAME=mon-recoweb-dev
# VERSION=2.0.0
# DESCRIPTION=Recon web KISS en 3 phases (dirb + ffuf dirs + ffuf files) avec exports JSON fiables et un résumé global agrégé (/répertoire/ vs /fichier). Pas d’auto-relance/benchmark : on alerte clairement en cas de 0 résultat ou d’incohérence dirb vs ffuf.
# PRESENTATION_START
# **mon-recoweb-dev — Recon web KISS (dirb + 2x ffuf)**
#
# Ce script enchaîne une énumération simple et efficace en 3 phases :
# - dirb (wordlist courte) pour une première cartographie rapide
# - ffuf avec `raft-medium-directories.txt` pour détecter un maximum de répertoires
# - ffuf avec `raft-medium-files.txt` pour détecter un maximum de fichiers
#
# KISS v3.0.0 :
# - Suppression benchmark/rate auto/relance auto : exécution “pure” et prévisible
# - Alertes claires si 0 résultat ffuf dirs/files
# - Alerte si dirb a trouvé des fichiers mais ffuf files = 0 (responsabilité utilisateur en exploitation)
#
# Sorties :
# - JSON ffuf (source de vérité)
# - TXT “style console” (si `jq` disponible) + hits-only
# - dirb.log + dirb_hits.txt
# - RESULTS_SUMMARY.txt : résumé global agrégé (format strict /répertoire/ et /fichier)
#
# Usage typique :
# `mon-recoweb-dev dog.htb`
# PRESENTATION_END
# HOMEPAGE=https://github.com/NoelNac-HackEthical/mes-scripts
#____________________________________________________________________________
#
# Bref résumé :
#   Recon web KISS en 3 phases (dirb + ffuf dirs + ffuf files) avec exports JSON fiables et un résumé global agrégé (/répertoire/ vs /fichier).
#
set -euo pipefail

# ---------------------------------------------------------------------------
# Helpers version (ne pas modifier)
_self_path="${BASH_SOURCE[0]:-$0}"
if command -v readlink >/dev/null 2>&1; then
  _resolved="$(readlink -f -- "$_self_path" 2>/dev/null || true)"
  [ -n "$_resolved" ] && _self_path="$_resolved"
fi
_self_base="$(basename "$_self_path")"

_version_str(){
  local v
  v="$(awk -F= '/^# *VERSION *=/ { gsub(/\r$/,"",$2); print $2; exit }' "$_self_path" 2>/dev/null || true)"
  v="${v:-0.0.0}"
  printf '%s v%s\n' "$_self_base" "$v"
}
_print_version_and_exit(){ _version_str; exit 0; }
# ---------------------------------------------------------------------------

usage(){
  cat <<USAGE
Usage: ${_self_base} [OPTIONS] <target>
       ${_self_base} [OPTIONS] -t <target>

Description:
  Recon web KISS en 3 phases (dirb + ffuf dirs + ffuf files), avec exports JSON fiables et résumé global agrégé.

Target:
  <target>                  Host (ex: dog.htb) OU URL complète (ex: http://10.10.10.10)
  -t, --target <target>     (Optionnel) même chose, pour compatibilité

Options:
  -s, --scheme <http|https> Scheme si target est un host (défaut: http)
  --base <path>             Base path prefix (défaut: /) ex: /app => http://t/app/FUZZ
  -T, --threads <n>         Threads ffuf (défaut: 30)
  --rate <n>                Limite le débit ffuf (req/s). Ex: 40, 100, 200 (défaut: none)
  --timeout <sec>           Timeout ffuf en secondes (défaut: 10)
  --fc <codes>              Filtre codes status ffuf (défaut: 404)
  --dirb-wordlist <path>    Wordlist dirb (défaut: /usr/share/wordlists/dirb/common.txt)
  --raft-dirs <path>        Wordlist ffuf dirs (défaut: /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt)
  --raft-files <path>       Wordlist ffuf files (défaut: /usr/share/seclists/Discovery/Web-Content/raft-medium-files.txt)
  -o, --outdir <dir>        Dossier de sortie (défaut: scans_recoweb)
  --no-dirb                 Skip dirb
  --no-ffuf-dirs            Skip ffuf directories
  --no-ffuf-files           Skip ffuf files
  --debug                   Debug (set -x)
  -h, --help                Help
  -V, --version             Version
USAGE
}

# ---------------------------------------------------------------------------
# Parsing minimal CLI (options AVANT ou APRES la cible)
DEBUG=false

if [[ "${1:-}" == "--version" || "${1:-}" == "-V" ]]; then _print_version_and_exit; fi
if [[ "${1:-}" == "--help" || "${1:-}" == "-h" ]]; then _version_str; usage; exit 0; fi

TARGET=""
SCHEME="http"
BASE="/"
THREADS=30
RATE=""          # limite débit ffuf (req/s). Vide = none
TIMEOUT=10
FFUF_FC="404"

DIRB_WORDLIST="/usr/share/wordlists/dirb/common.txt"
RAFT_DIRS="/usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt"
RAFT_FILES="/usr/share/seclists/Discovery/Web-Content/raft-medium-files.txt"

OUTDIR="scans_recoweb"
DO_DIRB=true
DO_FFUF_DIRS=true
DO_FFUF_FILES=true

while [[ $# -gt 0 ]]; do
  case "$1" in
    --debug) DEBUG=true; shift ;;
    -V|--version) _print_version_and_exit ;;
    -h|--help) _version_str; usage; exit 0 ;;

    -t|--target) TARGET="${2:-}"; shift 2 ;;
    -s|--scheme) SCHEME="${2:-}"; shift 2 ;;
    --base) BASE="${2:-}"; shift 2 ;;
    -T|--threads) THREADS="${2:-}"; shift 2 ;;
    --rate) RATE="${2:-}"; shift 2 ;;
    --timeout) TIMEOUT="${2:-}"; shift 2 ;;
    --fc) FFUF_FC="${2:-}"; shift 2 ;;
    --dirb-wordlist) DIRB_WORDLIST="${2:-}"; shift 2 ;;
    --raft-dirs) RAFT_DIRS="${2:-}"; shift 2 ;;
    --raft-files) RAFT_FILES="${2:-}"; shift 2 ;;
    -o|--outdir) OUTDIR="${2:-}"; shift 2 ;;
    --no-dirb) DO_DIRB=false; shift ;;
    --no-ffuf-dirs) DO_FFUF_DIRS=false; shift ;;
    --no-ffuf-files) DO_FFUF_FILES=false; shift ;;
    --) shift; break ;;

    -*) echo "Unknown option: $1" >&2; usage; exit 2 ;;

    *)
      if [[ -z "$TARGET" ]]; then
        TARGET="$1"; shift
      else
        echo "ERROR: argument inattendu: $1" >&2
        usage
        exit 2
      fi
      ;;
  esac
done
# ---------------------------------------------------------------------------

_need_cmd(){ command -v "$1" >/dev/null 2>&1 || { echo "ERROR: missing command: $1" >&2; exit 127; }; }
_need_file(){ [ -f "$1" ] || { echo "ERROR: missing file: $1" >&2; exit 1; }; }

_norm_base(){
  local b="$1"
  [ -z "$b" ] && b="/"
  [[ "$b" != /* ]] && b="/$b"
  [[ "$b" != "/" ]] && b="${b%/}"
  printf '%s' "$b"
}

_is_full_url(){ [[ "$1" =~ ^https?:// ]]; }

_build_base_url(){
  local t="$1" s="$2" b="$3"
  b="$(_norm_base "$b")"
  if _is_full_url "$t"; then
    [[ "$b" == "/" ]] && echo "${t%/}" || echo "${t%/}${b}"
  else
    [[ "$b" == "/" ]] && echo "${s}://${t}" || echo "${s}://${t}${b}"
  fi
}

_check_target_accessible(){
  local t="$1"
  if _is_full_url "$t"; then
    curl -ksI --max-time 5 "$t" >/dev/null 2>&1
    return $?
  fi
  ping -c 1 -W 2 "$t" >/dev/null 2>&1
}

_detect_soft404_size(){
  local base_url="$1"
  local probe="__monrecoweb_probe_${RANDOM}_${RANDOM}__"
  local url="${base_url%/}/${probe}"

  local code
  code="$(curl -s -o /dev/null -w "%{http_code}" --max-time 5 "$url" 2>/dev/null || true)"

  if [[ "$code" == "200" ]]; then
    local size
    size="$(curl -s --max-time 5 "$url" 2>/dev/null | wc -c | tr -d ' ' || true)"
    [[ -n "$size" && "$size" =~ ^[0-9]+$ ]] && echo "$size"
  fi
}

_fmt_duration(){
  local s="${1:-0}"
  [[ "$s" =~ ^[0-9]+$ ]] || s=0
  local h=$((s/3600))
  local m=$(((s%3600)/60))
  local r=$((s%60))
  if (( h > 0 )); then
    printf "%dh%02dm%02ds" "$h" "$m" "$r"
  elif (( m > 0 )); then
    printf "%dm%02ds" "$m" "$r"
  else
    printf "%ds" "$r"
  fi
}

_ffuf_export_txt_from_json(){
  local json="$1" out_txt="$2" out_hits="$3"
  if command -v jq >/dev/null 2>&1; then
    {
      echo "ffuf results"
      echo
      echo "Generated: $(date)"
      echo "Source   : $json"
      echo "------------------------------------------------"
      echo
      jq -r '
        .results[]
        | "\(.input.FUZZ)\t[Status: \(.status), Size: \(.length), Words: \(.words), Lines: \(.lines), Duration: \(.duration)]"
      ' "$json" | column -t -s $'\t'
    } > "$out_txt"

    jq -r '
      .results[]
      | "\(.input.FUZZ)\t[Status: \(.status), Size: \(.length), Words: \(.words), Lines: \(.lines), Duration: \(.duration)]"
    ' "$json" | column -t -s $'\t' > "$out_hits"
  else
    : > "$out_txt"
    : > "$out_hits"
  fi
}

_warn_ffuf_zero_hits(){
  # Args: label json_file
  local label="$1" json="$2"
  command -v jq >/dev/null 2>&1 || return 0
  [[ -s "$json" ]] || return 0

  local n
  n="$(jq '.results | length' "$json" 2>/dev/null || echo 0)"
  if [[ "${n:-0}" -eq 0 ]]; then
    printf "%b\n" "${C_ALERT}[!] ALERT:${C_RESET} ffuf ${label} → 0 résultat"
    printf "%b\n" "${C_WARN}[!] Suggestion:${C_RESET} vérifier 2-3 fichiers clés à la main (robots.txt, sitemap.xml) et ajuster threads/rate/filters."
  fi
}

_warn_dirb_files_but_ffuf_files_zero(){
  # Args: dirb_hits json_files
  local dirb_hits="$1" json_files="$2"
  command -v jq >/dev/null 2>&1 || return 0
  [[ -s "$dirb_hits" && -s "$json_files" ]] || return 0

  local dirb_files ffuf_files
  dirb_files="$(awk 'NF && $0 !~ /\/$/' "$dirb_hits" | wc -l | tr -d ' ' || true)"
  ffuf_files="$(jq '.results | length' "$json_files" 2>/dev/null || echo 0)"

  if [[ "${dirb_files:-0}" -gt 0 && "${ffuf_files:-0}" -eq 0 ]]; then
    printf "%b\n" "${C_ALERT}[!] ALERT:${C_RESET} dirb a trouvé des fichiers (${dirb_files}) MAIS ffuf files = 0"
    printf "%b\n" "${C_INFO}[!] Interprétation:${C_RESET} wildcard/soft-404 variable, filtres trop stricts, ou serveur qui réagit différemment."
    printf "%b\n" "${C_WARN}[!] Action:${C_RESET} retester les URLs de dirb_hits.txt au curl, puis ajuster ffuf (t/rate/-fs/-fw/-fl)."
  fi
}

# --- Couleurs (si stdout est un terminal) ---
if [[ -t 1 ]]; then
  C_ALERT=$'\e[1;31m'   # rouge vif
  C_WARN=$'\e[1;33m'    # jaune
  C_INFO=$'\e[1;36m'    # cyan
  C_RESET=$'\e[0m'
else
  C_ALERT=""
  C_WARN=""
  C_INFO=""
  C_RESET=""
fi

_main(){
  if [ "$DEBUG" = true ]; then set -x; fi

  echo "Script: $(_version_str)"

  if [[ -z "$TARGET" ]]; then
    echo "ERROR: missing target (use -t <target> or provide it as positional argument)" >&2
    usage
    exit 2
  fi

  _need_cmd ffuf
  _need_cmd curl

  echo "[*] Test d'accessibilité de la cible"
  if ! _check_target_accessible "$TARGET"; then
    echo "[!] Cible inaccessible : $TARGET"
    echo "[*] Arrêt du script"
    exit 1
  fi
  echo "[+] Cible accessible"

  if $DO_DIRB; then
    _need_cmd dirb
    _need_file "$DIRB_WORDLIST"
  fi
  if $DO_FFUF_DIRS || $DO_FFUF_FILES; then
    _need_file "$RAFT_DIRS"
    _need_file "$RAFT_FILES"
  fi

  BASE="$(_norm_base "$BASE")"
  local BASE_URL
  BASE_URL="$(_build_base_url "$TARGET" "$SCHEME" "$BASE")"

  local SOFT404_SIZE=""
  SOFT404_SIZE="$(_detect_soft404_size "$BASE_URL")"
  if [[ -n "$SOFT404_SIZE" ]]; then
    echo "[*] Soft-404 détectée : 200 sur inexistant → ffuf utilisera -fs $SOFT404_SIZE"
    echo "[!] ATTENTION : si le contenu varie (taille instable), tu peux rater des hits."
    echo "[!] Conseil : en phase exploitation, reteste les fichiers importants (robots.txt, sitemap.xml, etc.) au curl."
  fi

  mkdir -p "$OUTDIR"

  local RATE_EFFECTIVE="$RATE"

  echo "[*] target : $TARGET"
  echo "[*] base   : $BASE_URL"
  echo "[*] outdir : $OUTDIR"
  if [[ -n "$RATE_EFFECTIVE" ]]; then
    echo "[*] ffuf   : threads=$THREADS rate=$RATE_EFFECTIVE timeout=${TIMEOUT}s fc=$FFUF_FC"
  else
    echo "[*] ffuf   : threads=$THREADS timeout=${TIMEOUT}s fc=$FFUF_FC"
  fi

  local LOG_DIRB="$OUTDIR/dirb.log"
  local LOG_FFUF_DIRS="$OUTDIR/ffuf_dirs.log"
  local LOG_FFUF_FILES="$OUTDIR/ffuf_files.log"
  local HITS_DIRB="$OUTDIR/dirb_hits.txt"

  local JSON_DIRS="$OUTDIR/ffuf_dirs.json"
  local JSON_FILES="$OUTDIR/ffuf_files.json"

  local TXT_DIRS="$OUTDIR/ffuf_dirs.txt"
  local HITS_DIRS="$OUTDIR/ffuf_dirs_hits.txt"
  local TXT_FILES="$OUTDIR/ffuf_files.txt"
  local HITS_FILES="$OUTDIR/ffuf_files_hits.txt"

  local SUMMARY_FILE="$OUTDIR/RESULTS_SUMMARY.txt"

  # Phase 1: dirb
  if $DO_DIRB; then
    echo
    echo "[+] Phase 1/3: dirb (common.txt)"
    dirb "${BASE_URL%/}/" "$DIRB_WORDLIST" -r | tee "$LOG_DIRB" || true

    tr -d '\r' < "$LOG_DIRB" \
    | sed -r 's/\x1B\[[0-9;]*[A-Za-z]//g' \
    | awk '
        match($0, /\+ (https?:\/\/[^ ]+)/, m) { print m[1]; next }
        match($0, /==> DIRECTORY: (https?:\/\/[^ ]+)/, m) { print m[1]; next }
      ' \
    | sort -u > "$HITS_DIRB" || true
  else
    echo
    echo "[!] Phase 1/3: dirb skipped (--no-dirb)"
    : > "$LOG_DIRB"
    : > "$HITS_DIRB"
  fi

  # Phase 2: ffuf dirs
  if $DO_FFUF_DIRS; then
    echo
    echo "[+] Phase 2/3: ffuf directories (raft-medium-directories)"
    ffuf -u "${BASE_URL%/}/FUZZ" \
      -w "$RAFT_DIRS" \
      -t "$THREADS" \
      ${RATE_EFFECTIVE:+-rate "$RATE_EFFECTIVE"} \
      -timeout "$TIMEOUT" \
      -fc "$FFUF_FC" \
      ${SOFT404_SIZE:+-fs "$SOFT404_SIZE"} \
      -of json -o "$JSON_DIRS" \
      2>&1 | tee "$LOG_FFUF_DIRS"

    _ffuf_export_txt_from_json "$JSON_DIRS" "$TXT_DIRS" "$HITS_DIRS"

    echo
    _warn_ffuf_zero_hits "directories" "$JSON_DIRS"
  else
    echo
    echo "[!] Phase 2/3: ffuf dirs skipped (--no-ffuf-dirs)"
    : > "$LOG_FFUF_DIRS"
    : > "$JSON_DIRS"
    : > "$TXT_DIRS"
    : > "$HITS_DIRS"
  fi

  # Phase 3: ffuf files (KISS: pas de benchmark / pas d'auto-relance)
  if $DO_FFUF_FILES; then
    echo
    echo "[+] Phase 3/3: ffuf files (raft-medium-files)"
    ffuf -u "${BASE_URL%/}/FUZZ" \
      -w "$RAFT_FILES" \
      -t "$THREADS" \
      ${RATE_EFFECTIVE:+-rate "$RATE_EFFECTIVE"} \
      -timeout "$TIMEOUT" \
      -fc "$FFUF_FC" \
      ${SOFT404_SIZE:+-fs "$SOFT404_SIZE"} \
      -of json -o "$JSON_FILES" \
      2>&1 | tee "$LOG_FFUF_FILES"

    _ffuf_export_txt_from_json "$JSON_FILES" "$TXT_FILES" "$HITS_FILES"

    echo
    _warn_ffuf_zero_hits "files" "$JSON_FILES"
    _warn_dirb_files_but_ffuf_files_zero "$HITS_DIRB" "$JSON_FILES"
  else
    echo
    echo "[!] Phase 3/3: ffuf files skipped (--no-ffuf-files)"
    : > "$LOG_FFUF_FILES"
    : > "$JSON_FILES"
    : > "$TXT_FILES"
    : > "$HITS_FILES"
  fi

  # Résumé global
  {
    echo "=== Résultat global (agrégé) ==="
    echo

    {
      if [[ -s "$HITS_DIRB" ]]; then
        sed -E 's#^https?://[^/]+##' "$HITS_DIRB" \
        | awk 'NF==0{next} $0!~/^\//{$0="/"$0}{print $0}'
      fi

      if [[ -s "$HITS_DIRS" ]]; then
        awk 'NF==0{next}{p=$1;if(p!~/^\//)p="/"p;if(p!~/\/$/)p=p"/";print p}' "$HITS_DIRS"
      fi

      if [[ -s "$HITS_FILES" ]]; then
        awk 'NF==0{next}{p=$1;if(p!~/^\//)p="/"p;sub(/\/$/,"",p);print p}' "$HITS_FILES"
      fi
    } | sed '/^$/d' | sort -u

    echo
    echo "=== Détails par outil ==="
    echo

    echo "[DIRB]"
    if [[ -s "$HITS_DIRB" ]]; then
      sed -E 's#^https?://[^/]+##' "$HITS_DIRB" \
      | awk 'NF==0{next} $0!~/^\//{$0="/"$0}{print $0}' \
      | sort -u
    else
      echo "(aucun résultat)"
    fi

    echo
    echo "[FFUF — DIRECTORIES]"
    if [[ -s "$HITS_DIRS" ]]; then
      awk 'NF==0{next}{p=$1;if(p!~/^\//)p="/"p;if(p!~/\/$/)p=p"/";print p}' "$HITS_DIRS" | sort -u
    else
      echo "(aucun résultat)"
    fi

    echo
    echo "[FFUF — FILES]"
    if [[ -s "$HITS_FILES" ]]; then
      awk 'NF==0{next}{p=$1;if(p!~/^\//)p="/"p;sub(/\/$/,"",p);print p}' "$HITS_FILES" | sort -u
    else
      echo "(aucun résultat)"
    fi

  } > "$SUMMARY_FILE"

  echo
  echo "[+] Outputs:"
  echo "    - $LOG_DIRB"
  echo "    - $HITS_DIRB"
  echo "    - $JSON_DIRS"
  echo "    - $TXT_DIRS"
  echo "    - $HITS_DIRS"
  echo "    - $JSON_FILES"
  echo "    - $TXT_FILES"
  echo "    - $HITS_FILES"
  echo "    - $SUMMARY_FILE"

  echo
  echo "[+] Résultat global:"
  echo
  sed 's/^/  /' "$SUMMARY_FILE"

  echo
  echo "[OK] Done."
}

if [[ "${BASH_SOURCE[0]}" == "$0" ]]; then
  _main "$@"
fi

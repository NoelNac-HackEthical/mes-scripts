#!/usr/bin/env bash
# NAME=mon-recoweb-dev
# VERSION=2.1.0
# DESCRIPTION=Reconnaissance web automatisée en 3 phases (dirb + ffuf directories + ffuf files) avec exports JSON ffuf fiables, détection soft-404 et résumé global agrégé. Supporte aussi <host>/<path>/ (scan ciblé d’un répertoire) et --ext pour tester des extensions en phase files. Le résumé affiche des URLs complètes avec (CODE:xxx|SIZE:yyy) quand disponible.
# PRESENTATION_START
# **mon-recoweb-dev — Recon web en 3 phases (dirb + ffuf dirs + ffuf files)**
#
# Objectif :
# - Obtenir une cartographie exploitable des répertoires et fichiers HTTP d’une cible CTF.
#
# Validation de l’accessibilité de la cible (sans ICMP) :
# - Si la cible est une URL complète : test HTTP(S) HEAD via curl
# - Si la cible est un host : test HTTP(S) sur la base URL via curl
# - Fallback TCP via nc sur les ports 80 et 443
#
# Phases d’énumération :
# 1) dirb (wordlist courte) → cartographie initiale + extraction des hits
# 2) ffuf directories (raft-medium-directories)
# 3) ffuf files (raft-medium-files) + option --ext
#
# Scan ciblé d’un répertoire :
# - La cible peut inclure un chemin : <host>/<path>/ (ex: cible.htb/cgi-bin/)
# - Le script ne scanne alors QUE ce répertoire (BASE=/path)
# - Les sorties sont écrites dans scans_recoweb/<dernier-segment>/ (ex: scans_recoweb/cgi-bin/)
#
# Wordlist "files" par défaut en scan de répertoire :
# - Si BASE != "/" ET que l’utilisateur n’a pas fourni --raft-files,
#   alors la phase ffuf files utilise par défaut : /usr/share/wordlists/dirb/common.txt
#   (utile pour les répertoires contenant des scripts / endpoints legacy).
#
# Gestion des soft-404 :
# - Détection automatique d’un HTTP 200 sur ressource inexistante
# - Calcul de la taille de réponse et application de -fs <size> à ffuf
#
# Sorties (dans --outdir, défaut : scans_recoweb) :
# - dirb.log + dirb_hits.txt
# - ffuf_dirs.json + ffuf_dirs.log
# - ffuf_files.json + ffuf_files.log
# - ffuf_dirs.txt / ffuf_dirs_hits.txt (si `jq` est disponible)
# - ffuf_files.txt / ffuf_files_hits.txt (si `jq` est disponible)
# - RESULTS_SUMMARY.txt : résumé global (URLs complètes + CODE/SIZE quand possible)
#
# Alertes (si `jq` est disponible) :
# - ffuf directories = 0 résultat
# - ffuf files = 0 résultat
# - dirb a trouvé des fichiers mais ffuf files = 0 (wildcard, soft-404 variable, filtres trop stricts)
#
# Usage typique :
# - Scan de base : ${_self_base} cible.htb
# - Scan ciblé répertoire + extensions : ${_self_base} cible.htb/chemin/ --ext ".sh,.cgi,.pl"
# PRESENTATION_END
# HOMEPAGE=https://github.com/NoelNac-HackEthical/mes-scripts
#____________________________________________________________________________
#
# Bref résumé :
#   Reconnaissance web automatisée en 3 phases avec dirb et ffuf, détection soft-404 et résumé global.
#
set -euo pipefail

# ---------------------------------------------------------------------------
# Helpers version (ne pas modifier)
_self_path="${BASH_SOURCE[0]:-$0}"
if command -v readlink >/dev/null 2>&1; then
  _resolved="$(readlink -f -- "$_self_path" 2>/dev/null || true)"
  [ -n "$_resolved" ] && _self_path="$_resolved"
fi
_self_base="$(basename "$_self_path")"

_version_str(){
  local v
  v="$(awk -F= '/^# *VERSION *=/ { gsub(/\r$/,"",$2); print $2; exit }' "$_self_path" 2>/dev/null || true)"
  v="${v:-0.0.0}"
  printf '%s v%s\n' "$_self_base" "$v"
}
_print_version_and_exit(){ _version_str; exit 0; }
# ---------------------------------------------------------------------------

usage(){
  cat <<USAGE
Usage: ${_self_base} [OPTIONS] <target>
       ${_self_base} [OPTIONS] -t <target>

Description:
  Recon web KISS en 3 phases (dirb + ffuf dirs + ffuf files), avec exports JSON fiables et résumé global.

Target:
  <target>                  Host (ex: dog.htb) OU URL complète (ex: http://10.10.10.10)
                            Supporte aussi host/path (ex: cible.htb/cgi-bin/)
  -t, --target <target>     (Optionnel) même chose, pour compatibilité

Options:
  -s, --scheme <http|https> Scheme si target est un host (défaut: http)
  --base <path>             Base path prefix (défaut: /) ex: /app => http://t/app/FUZZ
  --ext <list>              Extensions pour ffuf files (ex: ".sh,.cgi,.pl" ou "sh,cgi,pl")
  -T, --threads <n>         Threads ffuf (défaut: 30)
  --rate <n>                Limite le débit ffuf (req/s). Ex: 40, 100, 200 (défaut: none)
  --timeout <sec>           Timeout ffuf en secondes (défaut: 10)
  --fc <codes>              Filtre codes status ffuf (défaut: 404)
  --ffuf-extra <opts>       Options supplémentaires passées telles quelles à ffuf (dirs ET files)
                            Ex: --ffuf-extra "-fs 612"  ou  --ffuf-extra "-fs 612 -fw 10"
  --dirb-wordlist <path>    Wordlist dirb (défaut: /usr/share/wordlists/dirb/common.txt)
  --raft-dirs <path>        Wordlist ffuf dirs (défaut: /usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt)
  --raft-files <path>       Wordlist ffuf files (défaut: /usr/share/seclists/Discovery/Web-Content/raft-medium-files.txt)
                            Note: en scan ciblé de répertoire (BASE != /) et si --raft-files n'est pas fourni,
                                  la wordlist par défaut bascule automatiquement vers /usr/share/wordlists/dirb/common.txt
  -o, --outdir <dir>        Dossier de sortie (défaut: scans_recoweb)
  --no-dirb                 Skip dirb
  --no-ffuf-dirs            Skip ffuf directories
  --no-ffuf-files           Skip ffuf files
  --debug                   Debug (set -x)
  -h, --help                Help
  -V, --version             Version
USAGE
}

# ---------------------------------------------------------------------------
# Parsing minimal CLI (options AVANT ou APRES la cible)
DEBUG=false

if [[ "${1:-}" == "--version" || "${1:-}" == "-V" ]]; then _print_version_and_exit; fi
if [[ "${1:-}" == "--help" || "${1:-}" == "-h" ]]; then _version_str; usage; exit 0; fi

TARGET=""
SCHEME="http"
BASE="/"
THREADS=30
RATE=""          # limite débit ffuf (req/s). Vide = none
TIMEOUT=10
FFUF_FC="404"
FFUF_EXTRA_OPTS=""

EXT_RAW=""
FFUF_EXTS=""     # format ffuf: ".sh,.cgi,.pl"

DIRB_WORDLIST="/usr/share/wordlists/dirb/common.txt"
RAFT_DIRS="/usr/share/seclists/Discovery/Web-Content/raft-medium-directories.txt"
RAFT_FILES="/usr/share/seclists/Discovery/Web-Content/raft-medium-files.txt"
RAFT_FILES_EXPLICIT=false

OUTDIR="scans_recoweb"
DO_DIRB=true
DO_FFUF_DIRS=true
DO_FFUF_FILES=true

while [[ $# -gt 0 ]]; do
  case "$1" in
    --debug) DEBUG=true; shift ;;
    -V|--version) _print_version_and_exit ;;
    -h|--help) _version_str; usage; exit 0 ;;

    -t|--target) TARGET="${2:-}"; shift 2 ;;
    -s|--scheme) SCHEME="${2:-}"; shift 2 ;;
    --base) BASE="${2:-}"; shift 2 ;;
    --ext) EXT_RAW="${2:-}"; shift 2 ;;
    -T|--threads) THREADS="${2:-}"; shift 2 ;;
    --rate) RATE="${2:-}"; shift 2 ;;
    --timeout) TIMEOUT="${2:-}"; shift 2 ;;
    --fc) FFUF_FC="${2:-}"; shift 2 ;;
    --ffuf-extra) FFUF_EXTRA_OPTS="${2:-}"; shift 2 ;;
    --dirb-wordlist) DIRB_WORDLIST="${2:-}"; shift 2 ;;
    --raft-dirs) RAFT_DIRS="${2:-}"; shift 2 ;;
    --raft-files)
      RAFT_FILES="${2:-}"
      RAFT_FILES_EXPLICIT=true
      shift 2
      ;;
    -o|--outdir) OUTDIR="${2:-}"; shift 2 ;;
    --no-dirb) DO_DIRB=false; shift ;;
    --no-ffuf-dirs) DO_FFUF_DIRS=false; shift ;;
    --no-ffuf-files) DO_FFUF_FILES=false; shift ;;
    --) shift; break ;;

    -*) echo "Unknown option: $1" >&2; usage; exit 2 ;;

    *)
      if [[ -z "$TARGET" ]]; then
        TARGET="$1"; shift
      else
        echo "ERROR: argument inattendu: $1" >&2
        usage
        exit 2
      fi
      ;;
  esac
done
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# Normalisation --ext => format ffuf -e ".sh,.cgi,.pl"
if [[ -n "${EXT_RAW:-}" ]]; then
  EXT_RAW="${EXT_RAW// /}"
  IFS=',' read -r -a _ext_arr <<< "$EXT_RAW"
  _out=()
  for e in "${_ext_arr[@]}"; do
    [[ -n "$e" ]] || continue
    [[ "$e" == .* ]] || e=".$e"
    _out+=("$e")
  done
  FFUF_EXTS="$(IFS=,; echo "${_out[*]}")"
  if [[ -z "$FFUF_EXTS" ]]; then
    echo "ERROR: --ext fourni mais aucune extension valide après normalisation." >&2
    exit 2
  fi
fi
# ---------------------------------------------------------------------------

_need_cmd(){ command -v "$1" >/dev/null 2>&1 || { echo "ERROR: missing command: $1" >&2; exit 127; }; }
_need_file(){ [ -f "$1" ] || { echo "ERROR: missing file: $1" >&2; exit 1; }; }

_norm_base(){
  local b="$1"
  [ -z "$b" ] && b="/"
  [[ "$b" != /* ]] && b="/$b"
  [[ "$b" != "/" ]] && b="${b%/}"
  printf '%s' "$b"
}

_is_full_url(){ [[ "$1" =~ ^https?:// ]]; }

# Support <host>/<path>/ (sans scheme) : split en host + base
_split_host_and_path(){
  # imprime: "<host_or_url>\t<base_or_empty>"
  local t="$1"

  if _is_full_url "$t"; then
    printf "%s\t%s\n" "$t" ""
    return 0
  fi

  if [[ "$t" == */* ]]; then
    local host="${t%%/*}"
    local path="/${t#*/}"
    path="${path%/}"
    printf "%s\t%s\n" "$host" "$path"
    return 0
  fi

  printf "%s\t%s\n" "$t" ""
}

_build_base_url(){
  local t="$1" s="$2" b="$3"
  b="$(_norm_base "$b")"
  if _is_full_url "$t"; then
    [[ "$b" == "/" ]] && echo "${t%/}" || echo "${t%/}${b}"
  else
    [[ "$b" == "/" ]] && echo "${s}://${t}" || echo "${s}://${t}${b}"
  fi
}

_check_target_accessible(){
  # Args: target base_url
  local t="$1"
  local base_url="${2:-}"

  if _is_full_url "$t"; then
    curl -ksI --max-time 5 "$t" >/dev/null 2>&1
    return $?
  fi

  # Tentative HTTP(s) d'abord (plus fiable que ICMP)
  if [[ -n "$base_url" ]]; then
    curl -ksI --max-time 5 "${base_url%/}/" >/dev/null 2>&1 && return 0
  fi

  # Fallback TCP (pré-check canonique HTB) : évite les faux négatifs ICMP
  nc -z -w2 "$t" 80  >/dev/null 2>&1 && return 0
  nc -z -w2 "$t" 443 >/dev/null 2>&1 && return 0
  return 1
}

_detect_soft404_size(){
  local base_url="$1"
  local probe="__monrecoweb_probe_${RANDOM}_${RANDOM}__"
  local url="${base_url%/}/${probe}"

  local code
  code="$(curl -s -o /dev/null -w "%{http_code}" --max-time 5 "$url" 2>/dev/null || true)"

  if [[ "$code" == "200" ]]; then
    local size
    size="$(curl -s --max-time 5 "$url" 2>/dev/null | wc -c | tr -d ' ' || true)"
    [[ -n "$size" && "$size" =~ ^[0-9]+$ ]] && echo "$size"
  fi
}

_fmt_duration(){
  local s="${1:-0}"
  [[ "$s" =~ ^[0-9]+$ ]] || s=0
  local h=$((s/3600))
  local m=$(((s%3600)/60))
  local r=$((s%60))
  if (( h > 0 )); then
    printf "%dh%02dm%02ds" "$h" "$m" "$r"
  elif (( m > 0 )); then
    printf "%dm%02ds" "$m" "$r"
  else
    printf "%ds" "$r"
  fi
}

_ffuf_export_txt_from_json(){
  local json="$1" out_txt="$2" out_hits="$3"
  if command -v jq >/dev/null 2>&1; then
    {
      echo "ffuf results"
      echo
      echo "Generated: $(date)"
      echo "Source   : $json"
      echo "------------------------------------------------"
      echo
      jq -r '
        .results[]
        | "\(.input.FUZZ)\t[Status: \(.status), Size: \(.length), Words: \(.words), Lines: \(.lines), Duration: \(.duration)]"
      ' "$json" | column -t -s $'\t'
    } > "$out_txt"

    jq -r '
      .results[]
      | "\(.input.FUZZ)\t[Status: \(.status), Size: \(.length), Words: \(.words), Lines: \(.lines), Duration: \(.duration)]"
    ' "$json" | column -t -s $'\t' > "$out_hits"
  else
    : > "$out_txt"
    : > "$out_hits"
  fi
}

_warn_ffuf_zero_hits(){
  # Args: label json_file
  local label="$1" json="$2"
  command -v jq >/dev/null 2>&1 || return 0
  [[ -s "$json" ]] || return 0

  local n
  n="$(jq '.results | length' "$json" 2>/dev/null || echo 0)"
  if [[ "${n:-0}" -eq 0 ]]; then
    printf "%b\n" "${C_ALERT}[!] ALERT:${C_RESET} ffuf ${label} → 0 résultat"
    printf "%b\n" "${C_WARN}[!] Suggestion:${C_RESET} vérifier 2-3 fichiers clés à la main (robots.txt, sitemap.xml) et ajuster threads/rate/filters."
  fi
}

_warn_dirb_files_but_ffuf_files_zero(){
  # Args: dirb_hits json_files
  local dirb_hits="$1" json_files="$2"
  command -v jq >/dev/null 2>&1 || return 0
  [[ -s "$dirb_hits" && -s "$json_files" ]] || return 0

  local dirb_files ffuf_files
  dirb_files="$(awk 'NF && $0 !~ /\/$/' "$dirb_hits" | wc -l | tr -d ' ' || true)"
  ffuf_files="$(jq '.results | length' "$json_files" 2>/dev/null || echo 0)"

  if [[ "${dirb_files:-0}" -gt 0 && "${ffuf_files:-0}" -eq 0 ]]; then
    printf "%b\n" "${C_ALERT}[!] ALERT:${C_RESET} dirb a trouvé des fichiers (${dirb_files}) MAIS ffuf files = 0"
    printf "%b\n" "${C_INFO}[!] Interprétation:${C_RESET} wildcard/soft-404 variable, filtres trop stricts, ou serveur qui réagit différemment."
    printf "%b\n" "${C_WARN}[!] Action:${C_RESET} retester les URLs de dirb_hits.txt au curl, puis ajuster ffuf (t/rate/-fs/-fw/-fl)."
  fi
}

_outdir_suffix_from_base(){
  local b="$1"
  b="$(_norm_base "$b")"
  [[ "$b" == "/" ]] && { echo ""; return 0; }
  local s="${b##*/}"
  s="${s//[^A-Za-z0-9._-]/_}"
  echo "$s"
}

# Variables attendues par write_results_header() (définies dans _main juste avant l'écriture du résumé)
# NAME VERSION CMDLINE NOW TARGET_HOST_HDR TARGET_SCOPE_HDR DIRB_CMD FFUF_DIRS_CMD FFUF_FILES_CMD

write_results_header() {
  local summary_file="$1"

  {
    echo "===== ${NAME} — RÉSUMÉ DES RÉSULTATS ====="
    echo "Commande principale : ${CMDLINE}"
    echo "Script              : ${NAME} v${VERSION}"
    echo
    echo "Cible        : ${TARGET_HOST_HDR}"
    echo "Périmètre    : ${TARGET_SCOPE_HDR}"
    echo "Date début   : ${NOW}"
    echo
    echo "Commandes exécutées (exactes) :"
    echo
    echo "[dirb — découverte initiale]"
    echo "${DIRB_CMD:-n/a}"
    echo
    echo "[ffuf — énumération des répertoires]"
    echo "${FFUF_DIRS_CMD:-n/a}"
    echo
    echo "[ffuf — énumération des fichiers]"
    echo "${FFUF_FILES_CMD:-n/a}"
    echo
    echo "Processus de génération des résultats :"
    echo "- Les sorties JSON produites par ffuf constituent la source de vérité."
    echo "- Les entrées pertinentes sont extraites via jq (URL, code HTTP, taille de réponse)."
    echo "- Les réponses assimilables à des soft-404 sont filtrées par comparaison des tailles et des codes HTTP."
    echo "- Les URLs finales sont reconstruites à partir du périmètre scanné (racine du site ou sous-répertoire ciblé)."
    echo "- Les résultats sont normalisés sous la forme :"
    echo "    http://cible/chemin (CODE:xxx|SIZE:yyy)"
    echo "- Les chemins sont ensuite classés par type :"
    echo "    • répertoires (/chemin/)"
    echo "    • fichiers (/chemin.ext)"
    echo "- Le fichier RESULTS_SUMMARY.txt est généré par agrégation finale, sans retraitement manuel,"
    echo "  garantissant la reproductibilité complète du scan."
    echo
    echo "----------------------------------------------------"
    echo
  } > "${summary_file}"
}

# --- Couleurs (si stdout est un terminal) ---
if [[ -t 1 ]]; then
  C_ALERT=$'\e[1;31m'   # rouge vif
  C_WARN=$'\e[1;33m'    # jaune
  C_INFO=$'\e[1;36m'    # cyan
  C_RESET=$'\e[0m'
else
  C_ALERT=""
  C_WARN=""
  C_INFO=""
  C_RESET=""
fi

_main(){
  if [ "$DEBUG" = true ]; then set -x; fi

  echo "Script: $(_version_str)"

  if [[ -z "$TARGET" ]]; then
    echo "ERROR: missing target (use -t <target> or provide it as positional argument)" >&2
    usage
    exit 2
  fi

  _need_cmd ffuf
  _need_cmd curl

  if $DO_DIRB; then
    _need_cmd dirb
    _need_file "$DIRB_WORDLIST"
  fi

  # Support "host/path" (ex: cible.htb/dir/)
  local split host base_from_target
  split="$(_split_host_and_path "$TARGET")"
  host="$(printf '%s' "$split" | cut -f1)"
  base_from_target="$(printf '%s' "$split" | cut -f2)"

  local TARGET_HOST="$host"

  # Si le target contient un path (host/path), il override BASE
  if [[ -n "$base_from_target" ]]; then
    BASE="$base_from_target"
  fi
  BASE="$(_norm_base "$BASE")"

  # Si scan ciblé d’un répertoire et --raft-files non fourni, bascule par défaut vers dirb/common.txt
  if [[ "$BASE" != "/" && "$RAFT_FILES_EXPLICIT" = false ]]; then
    RAFT_FILES="/usr/share/wordlists/dirb/common.txt"
  fi

  # Vérification des wordlists nécessaires selon les phases activées (après éventuel override RAFT_FILES)
  if $DO_FFUF_DIRS; then
    _need_file "$RAFT_DIRS"
  fi
  if $DO_FFUF_FILES; then
    _need_file "$RAFT_FILES"
  fi

  local BASE_URL
  if _is_full_url "$TARGET"; then
    BASE_URL="$(_build_base_url "$TARGET" "$SCHEME" "$BASE")"
  else
    BASE_URL="$(_build_base_url "$TARGET_HOST" "$SCHEME" "$BASE")"
  fi

  echo "[*] Test d'accessibilité de la cible"
  local ACCESS_T="$TARGET"
  _is_full_url "$TARGET" || ACCESS_T="$TARGET_HOST"
  if ! _check_target_accessible "$ACCESS_T" "$BASE_URL"; then
    echo "[!] Aucune réponse TCP immédiate depuis $ACCESS_T."
    echo "[!] L'IP de $ACCESS_T est peut être incorrecte (reset HTB ?)."
    echo "[!] Vérifie l'IP dans l'interface HTB"
    echo "[!] et /etc/hosts si tu utilises un nom de domaine."
    echo
    echo "[!] Arrêt du script."
    exit 1
  fi
  echo "[+] Cible accessible"

  local SOFT404_SIZE=""
  SOFT404_SIZE="$(_detect_soft404_size "$BASE_URL")"
  if [[ -n "$SOFT404_SIZE" ]]; then
    echo "[*] Soft-404 détectée : 200 sur inexistant → ffuf utilisera -fs $SOFT404_SIZE"
    echo "[!] ATTENTION : si le contenu varie (taille instable), tu peux rater des hits."
    echo "[!] Conseil : en phase exploitation, reteste les fichiers importants (robots.txt, sitemap.xml, etc.) au curl."
  fi

  # OUTDIR suffix selon BASE (ex: /cgi-bin => scans_recoweb/cgi-bin)
  local OUT_SUFFIX
  OUT_SUFFIX="$(_outdir_suffix_from_base "$BASE")"
  if [[ -n "$OUT_SUFFIX" ]]; then
    OUTDIR="${OUTDIR%/}/$OUT_SUFFIX"
  fi
  mkdir -p "$OUTDIR"

  local RATE_EFFECTIVE="$RATE"

  echo "[*] target : $TARGET"
  echo "[*] host   : $TARGET_HOST"
  echo "[*] base   : $BASE_URL"
  echo "[*] outdir : $OUTDIR"
  echo "[*] files wordlist : $RAFT_FILES"
  if [[ -n "$FFUF_EXTS" ]]; then
    echo "[*] ext    : $FFUF_EXTS"
  fi
  if [[ -n "$RATE_EFFECTIVE" ]]; then
    echo "[*] ffuf   : threads=$THREADS rate=$RATE_EFFECTIVE timeout=${TIMEOUT}s fc=$FFUF_FC"
  else
    echo "[*] ffuf   : threads=$THREADS timeout=${TIMEOUT}s fc=$FFUF_FC"
  fi

  if [[ -n "${FFUF_EXTRA_OPTS:-}" ]]; then
    echo "[*] ffuf extra : $FFUF_EXTRA_OPTS"
  fi

  local LOG_DIRB="$OUTDIR/dirb.log"
  local LOG_FFUF_DIRS="$OUTDIR/ffuf_dirs.log"
  local LOG_FFUF_FILES="$OUTDIR/ffuf_files.log"
  local HITS_DIRB="$OUTDIR/dirb_hits.txt"

  local JSON_DIRS="$OUTDIR/ffuf_dirs.json"
  local JSON_FILES="$OUTDIR/ffuf_files.json"

  local TXT_DIRS="$OUTDIR/ffuf_dirs.txt"
  local HITS_DIRS="$OUTDIR/ffuf_dirs_hits.txt"
  local TXT_FILES="$OUTDIR/ffuf_files.txt"
  local HITS_FILES="$OUTDIR/ffuf_files_hits.txt"

  local SUMMARY_FILE="$OUTDIR/RESULTS_SUMMARY.txt"

  # Phase 1: dirb
  if $DO_DIRB; then
    echo
    echo "[+] Phase 1/3: dirb (common.txt)"
    dirb "${BASE_URL%/}/" "$DIRB_WORDLIST" -r | tee "$LOG_DIRB" || true

    tr -d '\r' < "$LOG_DIRB" \
    | sed -r 's/\x1B\[[0-9;]*[A-Za-z]//g' \
    | awk '
        match($0, /\+ (https?:\/\/[^ ]+)/, m) { print m[1]; next }
        match($0, /==> DIRECTORY: (https?:\/\/[^ ]+)/, m) { print m[1]; next }
      ' \
    | sort -u > "$HITS_DIRB" || true
  else
    echo
    echo "[!] Phase 1/3: dirb skipped (--no-dirb)"
    : > "$LOG_DIRB"
    : > "$HITS_DIRB"
  fi

  # Phase 2: ffuf dirs
  if $DO_FFUF_DIRS; then
    echo
    echo "[+] Phase 2/3: ffuf directories (raft-medium-directories)"
    ffuf -u "${BASE_URL%/}/FUZZ" \
      -w "$RAFT_DIRS" \
      -t "$THREADS" \
      ${RATE_EFFECTIVE:+-rate "$RATE_EFFECTIVE"} \
      -timeout "$TIMEOUT" \
      -fc "$FFUF_FC" \
      ${SOFT404_SIZE:+-fs "$SOFT404_SIZE"} \
      ${FFUF_EXTRA_OPTS:+$FFUF_EXTRA_OPTS} \
      -of json -o "$JSON_DIRS" \
      2>&1 | tee "$LOG_FFUF_DIRS"

    _ffuf_export_txt_from_json "$JSON_DIRS" "$TXT_DIRS" "$HITS_DIRS"

    echo
    _warn_ffuf_zero_hits "directories" "$JSON_DIRS"
  else
    echo
    echo "[!] Phase 2/3: ffuf dirs skipped (--no-ffuf-dirs)"
    : > "$LOG_FFUF_DIRS"
    : > "$JSON_DIRS"
    : > "$TXT_DIRS"
    : > "$HITS_DIRS"
  fi

  # Phase 3: ffuf files (KISS: pas de benchmark / pas d'auto-relance)
  if $DO_FFUF_FILES; then
    echo
    echo "[+] Phase 3/3: ffuf files (raft-medium-files)"
    ffuf -u "${BASE_URL%/}/FUZZ" \
      -w "$RAFT_FILES" \
      -t "$THREADS" \
      ${RATE_EFFECTIVE:+-rate "$RATE_EFFECTIVE"} \
      -timeout "$TIMEOUT" \
      -fc "$FFUF_FC" \
      ${SOFT404_SIZE:+-fs "$SOFT404_SIZE"} \
      ${FFUF_EXTS:+-e "$FFUF_EXTS"} \
      -of json -o "$JSON_FILES" \
      ${FFUF_EXTRA_OPTS:+$FFUF_EXTRA_OPTS} \
      2>&1 | tee "$LOG_FFUF_FILES"

    _ffuf_export_txt_from_json "$JSON_FILES" "$TXT_FILES" "$HITS_FILES"

    echo
    _warn_ffuf_zero_hits "files" "$JSON_FILES"
    _warn_dirb_files_but_ffuf_files_zero "$HITS_DIRB" "$JSON_FILES"
  else
    echo
    echo "[!] Phase 3/3: ffuf files skipped (--no-ffuf-files)"
    : > "$LOG_FFUF_FILES"
    : > "$JSON_FILES"
    : > "$TXT_FILES"
    : > "$HITS_FILES"
  fi

  # Ligne d'en-tête du résumé (contexte du scan)
  local SUMMARY_TARGET
  if [[ "$BASE" == "/" ]]; then
    SUMMARY_TARGET="${TARGET_HOST}/"
  else
    SUMMARY_TARGET="${TARGET_HOST}${BASE}/"
  fi

  # -------------------------------------------------------------------------
  # Entête RESULTS_SUMMARY (audit-ready) — écrit UNE seule fois
  # IMPORTANT: pas de "local" ici, car write_results_header() lit des variables globales

  NOW="$(date '+%Y-%m-%d %H:%M:%S')"

  # Commande exacte (quoting fiable)
  CMDLINE="$(printf '%q ' "$0" "$@")"
  CMDLINE="${CMDLINE% }"

  # NAME / VERSION depuis l'entête du fichier (fallbacks)
  NAME="$(awk -F= '/^# *NAME *=/ { gsub(/\r$/,"",$2); print $2; exit }' "$_self_path" 2>/dev/null || true)"
  NAME="${NAME:-${_self_base}}"
  VERSION="$(awk -F= '/^# *VERSION *=/ { gsub(/\r$/,"",$2); print $2; exit }' "$_self_path" 2>/dev/null || true)"
  VERSION="${VERSION:-0.0.0}"

  TARGET_DISPLAY="${SUMMARY_TARGET}"

  # Cible / périmètre (valable pour scan racine OU scan de répertoire)
  TARGET_HOST_HDR="$TARGET_HOST"
  if [[ "$BASE" == "/" ]]; then
    TARGET_SCOPE_HDR="/"
  else
    TARGET_SCOPE_HDR="${BASE}/"
  fi

  # Commandes exactes (incluant options conditionnelles + tee)
  if $DO_DIRB; then
    _dirb_core="$(printf '%q ' dirb "${BASE_URL%/}/" "$DIRB_WORDLIST" -r)"
    _dirb_core="${_dirb_core% }"
    DIRB_CMD="${_dirb_core} | tee $(printf '%q' "$LOG_DIRB")"
  else
    DIRB_CMD="(dirb skipped --no-dirb)"
  fi

  if $DO_FFUF_DIRS; then
    _ffuf_dirs_core="$(printf '%q ' ffuf -u "${BASE_URL%/}/FUZZ" -w "$RAFT_DIRS" -t "$THREADS" \
      ${RATE_EFFECTIVE:+-rate "$RATE_EFFECTIVE"} \
      -timeout "$TIMEOUT" -fc "$FFUF_FC" \
      ${SOFT404_SIZE:+-fs "$SOFT404_SIZE"} \
      ${FFUF_EXTRA_OPTS:+$FFUF_EXTRA_OPTS} \
      -of json -o "$JSON_DIRS")"
    _ffuf_dirs_core="${_ffuf_dirs_core% }"
    FFUF_DIRS_CMD="${_ffuf_dirs_core} 2>&1 | tee $(printf '%q' "$LOG_FFUF_DIRS")"
  else
    FFUF_DIRS_CMD="(ffuf dirs skipped --no-ffuf-dirs)"
  fi

  if $DO_FFUF_FILES; then
    _ffuf_files_core="$(printf '%q ' ffuf -u "${BASE_URL%/}/FUZZ" -w "$RAFT_FILES" -t "$THREADS" \
      ${RATE_EFFECTIVE:+-rate "$RATE_EFFECTIVE"} \
      -timeout "$TIMEOUT" -fc "$FFUF_FC" \
      ${SOFT404_SIZE:+-fs "$SOFT404_SIZE"} \
      ${FFUF_EXTS:+-e "$FFUF_EXTS"} \
      ${FFUF_EXTRA_OPTS:+$FFUF_EXTRA_OPTS} \
      -of json -o "$JSON_FILES")"
    _ffuf_files_core="${_ffuf_files_core% }"
    FFUF_FILES_CMD="${_ffuf_files_core} 2>&1 | tee $(printf '%q' "$LOG_FFUF_FILES")"
  else
    FFUF_FILES_CMD="(ffuf files skipped --no-ffuf-files)"
  fi

  write_results_header "$SUMMARY_FILE"
  # -------------------------------------------------------------------------

  # Résumé global (APPEND : ne pas écraser l'entête)
  {
    echo "=== Résultat global (agrégé) ==="
    echo

    {
      # --- DIRB : relire le log pour récupérer CODE/SIZE quand présent ---
      if [[ -s "$LOG_DIRB" ]]; then
        tr -d '\r' < "$LOG_DIRB" \
        | sed -r 's/\x1B\[[0-9;]*[A-Za-z]//g' \
        | awk '
            match($0, /\+ (https?:\/\/[^ ]+) \(CODE:([0-9]+)\|SIZE:([0-9]+)\)/, m) {
              print m[1] " (CODE:" m[2] "|SIZE:" m[3] ")"
              next
            }
            match($0, /==> DIRECTORY: (https?:\/\/[^ ]+)/, m) {
              # Dirb ne donne pas CODE/SIZE ici → on garde l’URL brute
              print m[1]
              next
            }
          ' \
        | sort -u
      fi

      # --- FFUF DIRECTORIES : JSON → URL + CODE/SIZE ---
      if [[ -s "$JSON_DIRS" ]] && command -v jq >/dev/null 2>&1; then
        jq -r --arg base "${BASE_URL%/}/" '
          .results[]
          | "\($base)\(.input.FUZZ)/ (CODE:\(.status)|SIZE:\(.length))"
        ' "$JSON_DIRS" 2>/dev/null || true
      fi

      # --- FFUF FILES : JSON → URL + CODE/SIZE ---
      if [[ -s "$JSON_FILES" ]] && command -v jq >/dev/null 2>&1; then
        jq -r --arg base "${BASE_URL%/}/" '
          .results[]
          | "\($base)\(.input.FUZZ) (CODE:\(.status)|SIZE:\(.length))"
        ' "$JSON_FILES" 2>/dev/null || true
      fi

    } | sed '/^$/d' | sort -u

    echo
    echo "=== Détails par outil ==="
    echo

    echo "[DIRB]"
    if [[ -s "$LOG_DIRB" ]]; then
      tr -d '\r' < "$LOG_DIRB" \
      | sed -r 's/\x1B\[[0-9;]*[A-Za-z]//g' \
      | awk '
          match($0, /\+ (https?:\/\/[^ ]+) \(CODE:([0-9]+)\|SIZE:([0-9]+)\)/, m) {
            print m[1] " (CODE:" m[2] "|SIZE:" m[3] ")"
            next
          }
          match($0, /==> DIRECTORY: (https?:\/\/[^ ]+)/, m) { print m[1]; next }
        ' \
      | sort -u
    else
      echo "(aucun résultat)"
    fi

    echo
    echo "[FFUF — DIRECTORIES]"
    if [[ -s "$JSON_DIRS" ]] && command -v jq >/dev/null 2>&1; then
      jq -r --arg base "${BASE_URL%/}/" '
        .results[]
        | "\($base)\(.input.FUZZ)/ (CODE:\(.status)|SIZE:\(.length))"
      ' "$JSON_DIRS" 2>/dev/null | sort -u || true
    else
      echo "(aucun résultat)"
    fi

    echo
    echo "[FFUF — FILES]"
    if [[ -s "$JSON_FILES" ]] && command -v jq >/dev/null 2>&1; then
      jq -r --arg base "${BASE_URL%/}/" '
        .results[]
        | "\($base)\(.input.FUZZ) (CODE:\(.status)|SIZE:\(.length))"
      ' "$JSON_FILES" 2>/dev/null | sort -u || true
    else
      echo "(aucun résultat)"
    fi

  } >> "$SUMMARY_FILE"

  echo
  echo "[+] Outputs:"
  echo "    - $LOG_DIRB"
  echo "    - $HITS_DIRB"
  echo "    - $JSON_DIRS"
  echo "    - $TXT_DIRS"
  echo "    - $HITS_DIRS"
  echo "    - $JSON_FILES"
  echo "    - $TXT_FILES"
  echo "    - $HITS_FILES"
  echo "    - $SUMMARY_FILE"

  echo
  echo "[+] Résultat global:"
  echo
  sed 's/^/  /' "$SUMMARY_FILE"

  echo
  echo "[OK] Done."
}

if [[ "${BASH_SOURCE[0]}" == "$0" ]]; then
  _main "$@"
fi
